---
id: Probability-and-Statistics
slug: /Statistics
sidebar_position: 1
---

# Probability and Statistics

## Probability Spaces and Mesures
---
Let $\Omega$ is a non-empty set and $\mathfrak{R}$ is an $\sigma$-algebra generated by the space $\Omega$. A **probability measure** is a non-negative finite measure $\mathbb{P}$ defined on the $\mathfrak{R}$ with value in $[0,1]$, and $\mathbb{P}(\Omega)=1$. Then the structure of $(\Omega,\mathfrak{R},\mathbb{P})$ is called a probability space. 

Idealy, the space $\Omega$ contained all the possible outcomes in the project we concerns. The probability measure defined on the $\sigma$-algebra $\mathfrak{R}$ represents the probability $\mathbb{P}(A)$ to obtain the subset $A\subset \Omega$. 


## Random Variables and Distributions
---
Given a probability space $(\Omega, \mathfrak{R},\mathbb{P})$ where $\Omega$ is the event pool and $\mathfrak{R}$ is a $\sigma$-algebra defined on $\Omega$ and $\mathbb{P}$ is a probability measure defined on $\mathfrak{R}$ and $\mathbb{P}(\Omega)=1$. A random variable $X:\Omega\to \mathbb{R}$ is a $\mathfrak{R}$-measurable function which means for any Borel set $B\in\mathbb{R}$:
$$
X^{-1}(B)=\lbrace\omega|\omega\in\Omega, X(\omega)\in B\rbrace \in\mathfrak{R}.
$$

A **distriubtion measure** $\mu_X$ of $X$ is a pushforward measure induced by $X$ follows $\mu_X(B)=X_*\mathbb{P}(B)=\mathbb{P}[X^{-1}(B)]$. The Randon-Nikodym's theorem make sure a non-negative $f(x)$ exists such that
$$
\mu_X(B)=\int_Bf(x)dx,
$$
which is called the **probability density function** (PDF) and a **cumulated distribution function** (CDP) $F(x)$ is defined as $F(x)=\mathbb{P}\lbrace X\le x\rbrace$. 

A **characteristic function** $\varphi(t)$ for the random variable is defined as $\varphi(t)=\mathbb{E}(e^{iXt})$. Suppose $X=\sum_ia_iX_i$ where $X_i$ are mutually independent, then 
$$
\varphi_X(t)=\prod_i\varphi_{X_i}(a_it).
$$
Furthermore, a moment generate function is defined as $M(t)=\varphi(-it)$ and 
$$
\mathbb{E}(X^k)=\frac{d^kM(t)}{dt^k}\big|_{t=0}.
$$


## Joint Distributions
---

For two random variables $X,Y$, the direct product forms a map $X\otimes Y:\Omega\to \mathbb{R}\otimes\mathbb{R}$, and the corresponding measure is defined as
$$
\mu_{X\otimes Y}(B_1\otimes B_2)=\mathbb{P}[X^{-1}(B_1)\cap Y^{-1}(B_2)],
$$
where $B_1,B_2$ are arbitrary Borel sets in $\mathbb{R}$. The PDF or CDF incuded from this measure is called **joint probability density function** or **joint cumulative distribution function**. Usually, it is easier to construct the joint CDF, denoted as $F_{X,Y}(x,y)$ and the joint PDF denoting as $f_{X,Y}(x,y)$ is obtained by the partial differential. Furthermore, if $X\perp Y$ (independent), then
$$
\mathbb{P}\left\lbrace X^{-1}\big([-\infty,x]\big)\cap Y^{-1}\big([-\infty,y]\big)\right\rbrace =\mathbb{P}\left\lbrace X^{-1}\big([-\infty,x]\big)\right\rbrace \mathbb{P}\left\lbrace Y^{-1}\big([-\infty,y]\big)\right\rbrace,
$$ 
so that $F_{X,Y}(x,y)=F_X(x)F_Y(y)$ and so is the joint PDF. 

